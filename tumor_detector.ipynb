{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdce3096",
   "metadata": {},
   "source": [
    "#### Python Version\n",
    "This neural network runs on Python 3.12 to ensure compatability with its dependencies. If you are running this notebook in a virtual environment, ensure you have the correct runtime selected by running the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac792483",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99faf28f",
   "metadata": {},
   "source": [
    "#### Install Dependencies\n",
    "Installs the following dependencies for use in the notebook:\n",
    "* **Torch:** The model is built using the PyTorch framework (this is also what limits the Python version to <= 3.12)\n",
    "* **Torchvision:** Has functions for handling and preparing datasets for PyTorch models\n",
    "* **Kaggle:** Download datasets from an online repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b2a6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "%pip install torchvision\n",
    "%pip install kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b90d44b",
   "metadata": {},
   "source": [
    "#### Download and Prepare Datasets for Use\n",
    "> Prior to running this code block, ensure you have a Kaggle API key stored locally in `~/.kaggle/kaggle.json`. Visit the Kaggle website for information on how to acquire an API key.\n",
    "This neural network combines data from two datasets:\n",
    "* The Breast Ultrasound Images (BUSI) Dataset (Al-Dhabyani W, Gomaa M, Khaled H, Fahmy A. Dataset of breast ultrasound images. Data in Brief. 2020 Feb;28:104863. DOI: 10.1016/j.dib.2019.104863.)\n",
    "* Vuppala Adithya Sairam's Ultrasound Breat Images for Breast Cancer dataset, for which he has not provided a source other than the fact that it was aggregated from various open breast cancer ultrasound datasets\n",
    "\n",
    "The BUSI dataset had an additional \"normal\" class of ultrasounds that had no tumors, but these are deleted as the purpose of this model is to identify whether a detected tumor is malignant of benign. Both datasets have \"benign\" and \"malignant\" images which are aggregated together. Sairam's dataset was already split into test and evaluation datasets, but these were combined as this notebook randomly splits the datasets later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f43d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download \"aryashah2k/breast-ultrasound-images-dataset\" -p \"./data\"\n",
    "!unzip data/breast-ultrasound-images-dataset.zip\n",
    "!rm -rf Dataset_BUSI_with_GT/normal\n",
    "!mv Dataset_BUSI_with_GT/* data\n",
    "!rm -rf Dataset_BUSI_with_GT\n",
    "!rm -rf data/breast-ultrasound-images-dataset.zip\n",
    "!find data -type f -name \"*_mask*.png\" -delete\n",
    "\n",
    "!kaggle datasets download \"vuppalaadithyasairam/ultrasound-breast-images-for-breast-cancer\" -p \"./data\"\n",
    "!unzip data/ultrasound-breast-images-for-breast-cancer.zip\n",
    "!mv \"ultrasound breast classification/train/benign\"/* data/benign\n",
    "!mv \"ultrasound breast classification/val/benign\"/* data/benign\n",
    "!mv \"ultrasound breast classification/train/malignant\"/* data/malignant\n",
    "!mv \"ultrasound breast classification/val/malignant\"/* data/malignant\n",
    "!rm -rf \"ultrasound breast classification\"\n",
    "!rm -rf data/ultrasound-breast-images-for-breast-cancer.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f775dd27",
   "metadata": {},
   "source": [
    "#### Import Necessary Dependencies\n",
    "The following dependencies are imported:\n",
    "* `torch`: Contains various functions for identifying GPUs and manipulating Tensors\n",
    "* `torch.nn`: Contains various prebuilt layers and classes to develop a deep learning network\n",
    "* `torch.nn.functional`: Contains implementations of activation functions that add non-linearity to a deep learning network\n",
    "* `torch.optim`: Contains optimizers used in model training\n",
    "* `time`: Used in displaying metrics while training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee9d9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391a2eb1",
   "metadata": {},
   "source": [
    "#### Select Device for Training\n",
    "The following cell selects the best available device for training, testing, and performing inferences with the AI model. If a CUDA GPU is available, all calculations will be performed on the GPU. If an M-series Mac is used, PyTorch's MPS backend is used. Otherwise, all calculations will be done on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de716683",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3b7c54",
   "metadata": {},
   "source": [
    "#### Load and Transform Datasets\n",
    "The `torchvision` library is used to load and transform the data. The data is turned into a labeled dataset with the following labels:\n",
    "* Images in `data/benign` will have a label `0`\n",
    "* Images in `data/malignant` will have a label `1`\n",
    "\n",
    "Images in the dataset are also transformed. They are converted to Grayscale (ultrasounds are in black and white anyway, so training on 3 color channels is a waste of computation power), transformed to Tensor shapes, and normalized to have a mean and standard deviation of 0.5. The data set is then randomly split into a train and test dataset, with the train dataset containing `80%` of the original dataset and the test dataset containing the remaining `20%`. These two datasets are then loaded, with the training dataset being randomly shuffled every epoch.\n",
    "\n",
    "A batch size of `1` is used for both datasets as PyTorch expects batches to contain data of the same size. Since this model is designed to handle images of any size and the datasets are shuffled, a batch size of `1` is used to avoid any errors regarding data size mismatches within batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176e3778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "transform = v2.Compose([\n",
    "    v2.Grayscale(num_output_channels=1),\n",
    "    v2.RandomHorizontalFlip(0.5),\n",
    "    v2.RandomRotation(30),\n",
    "    v2.Resize((300, 300)),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=\"data\", transform=transform)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ad787f",
   "metadata": {},
   "source": [
    "#### Define Neural Network Architecture\n",
    "The model is a Convolutional Neural Network with Adaptive Average Pooling, meaning it can take in grayscale images of any size and identify a set number of features present within those images. These features are then passed through a fully connected neural network to determine whether an ultrasound contains a benign or malignant tumor.\n",
    "\n",
    "The model's design choices are as follows:\n",
    "* The convolutional portion of the model utilizes a pattern of two `3x3` `Conv2d` convolutional layers in a row followed by a `MaxPool2d` max pooling layer. The sequential `Conv2d` layers allow for powerful feature identification (when compared to a single convolutional layer, even one with a larger `5x5` kernel size) before being reduced to its more prominent characteristics through `MaxPool2d` layers.\n",
    "* 6 `Conv2d` layers going from `16` to `32` to `64` output features create a balance between preventing overfitting and extracting sufficient features to accuractly determine a tumor's maliciousness.\n",
    "* The `Conv2d` layers have a padding of `1` to retain the same output size as input size. This made the model more simple to develop, and doesn't consume crazy amounts of VRAM as these layers' are eventually adaptively pooled together through the `AdaptiveAvgPool2d` layer.\n",
    "    * The `Conv2d` outputs' sizes are only modified by the interspersed `MaxPool2d` layers, which halves their heights and widths.\n",
    "* The `AdaptiveAvgPool2d` layer bridges the convolutional and fully connected portions of the model. After inputs pass through the convolutional portion of the model and before they are pooled by the `AdaptiveAvgPool2d` layer, they have a shape of `64 x H/8 x W/8`, where `H` and `W` are the original heights and widths of the inputs, respectively. Adaptive average pooling reduces this to a `64 x 1 x 1` shape, with the resultant `64` features being a generalization of the convolutional layers' output.\n",
    "    * In effect, this allows the model to take in input grayscale ultrasound images of any dimension, as they're eventually reduced to a `64 x 1 x 1` shape.\n",
    "* The fully connected portion of the model is sparse and simple to prevent overfitting. It contains a single `Dropout` layer with a probability of `50%` between its two `Linear` layers to further improve the model's generalizability.\n",
    "* The activation function utilized throughout this network is the `LeakyReLU` activation function. It was discovered during the model's development, when `ReLU` was the activation function being utilized, that there was a potential of dead neurons impacting model accuracy. Switching to `LeakyReLU` with a negative slope of `0.01` prevented dead neurons from occurring and lead to a `~2%` accuracy gain.\n",
    "* The model outputs logits instead of probabilities through a `Sigmoid` activation layer, as the custom loss function defined for this model is more computationally efficient when model outputs are logits.\n",
    "    * However, these logit outputs can be passed through a `Sigmoid` activation function in the code handling inferences for user-friendly output as obviously loss functions are not of concern there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf5b599",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreastCancerCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BreastCancerCNN, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout2d(0.3),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout2d(0.3),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout2d(0.3),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.3)\n",
    "        )\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # Fully connected and dropout layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1bd2d8",
   "metadata": {},
   "source": [
    "#### Identifying Dataset Biases\n",
    "In the source images, there are fewer malignant tumor images than benign tumor images. Since the training and test datasets are randomly split fractions of the total ultrasound dataset, it's safe to assume that in the training data there are more instances of benign tumors than malignant. This cell is meant to verify that assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6d197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_count = 0\n",
    "malignant_count = 0\n",
    "for _, label in train_loader:\n",
    "    if label.item() == 0:\n",
    "        benign_count += 1\n",
    "    else:\n",
    "        malignant_count += 1\n",
    "\n",
    "print(f\"Benign Image Count: {benign_count}\")\n",
    "print(f\"Malignant Image Count: {malignant_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cd1848",
   "metadata": {},
   "source": [
    "#### Addressing Dataset Biases\n",
    "Because there tend to be fewer examples of malignant tumors, it's harder for the neural network to identify malignant tumors compared to benign ones. Regular cross-entropy or binary cross-entropy loss functions don't address this issue, but their variant focal loss does. The focal loss formula looks like this:\n",
    "$$\n",
    "FL(p_t) = -\\alpha_t(1-p_t)^\\gamma\\log(p_t)\n",
    "$$\n",
    "$\n",
    "\\newline p_t = \\text{Probability of the input being of label } t\n",
    "\\newline \\alpha_t = \\text{Hyperparameter from } [0, 1] \\text{ that scales down the loss of the label with fewer training instances. In binary classification tasks } \\alpha_t = \\alpha \\text{ if } p_t = p \\text{ and } \\alpha_t = (1 - \\alpha) \\text{ if } p_t = (1 - p)\n",
    "\\newline \\gamma = \\text{Hyperparameter } \\geq 0 \\text{ that scales down the loss of easily identifiable labels to focus on training harder ones}\n",
    "$\n",
    "\n",
    "When adapted for binary classification tasks, the focal loss formula can look like the following:\n",
    "$$\n",
    "FL(p, y) = -\\alpha y(1-p)^\\gamma\\log(p) - (1 - \\alpha) (1-y)(p)^\\gamma\\log(1-p)\n",
    "$$\n",
    "$\n",
    "\\newline y = \\text{Whether the label is 1 (malignant) or 0 (benign)}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c03661",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction=\"mean\"):\n",
    "        super(BinaryFocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        # Convert to float for Binary Cross Entropy\n",
    "        targets = targets.float()\n",
    "        cross_entropy_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
    "        # Sigmoid the inputs to convert them into probabilities\n",
    "        p = torch.sigmoid(inputs)\n",
    "        # Since targets is either 0 or 1, this returns the probability for each possible outcome as either targets or (1 - targets) is 0 in these equations\n",
    "        p_t = p * targets + (1 - p) * (1 - targets)\n",
    "        alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
    "        # Actually apply the focal loss formula\n",
    "        focal_loss = alpha_t * (1 - p_t) ** self.gamma * cross_entropy_loss\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc98e6e",
   "metadata": {},
   "source": [
    "#### Initialize Neural Network, Loss Function, Optimizer, and Epoch Counts\n",
    "The loss function chosen is the aforementioned `BinaryFocalLoss` function. Since the majority label is the `0` label (images of benign tumors), an `alpha` of `0.25` allows its $\\alpha_t = 0.75$ and thus its loss to be scaled by a factor of `0.75`, which is 3 times the scaling of the loss of the minority `1` label (images of malignant tumors), which has $\\alpha_t = 1 - 0.75 = 0.25$. A `gamma` of 4 was selected to aggressively account for the imbalance in benign and malignant tumor images, as its more important for a cancer screening model to identify malignant tumors than benign ones.\n",
    "\n",
    "The `AdamW` optimizer is selected as it adds effective handling of a small weight decay to prevent overfitting. A lower learning rate of `3e-4` was selected over the standard `1e-3` due to a need for increased accuracy.\n",
    "\n",
    "`40` epochs were selected to result in an improved accuracy over the previous `20` training epochs used while still not taking exhorbinant amounts of time to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c3dc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_net = BreastCancerCNN()\n",
    "cancer_net = cancer_net.to(device)\n",
    "loss_fn = BinaryFocalLoss(alpha=0.2, gamma=4)\n",
    "optimizer = optim.AdamW(cancer_net.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "num_epochs = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb67d8b3",
   "metadata": {},
   "source": [
    "#### Train the Model\n",
    "The below cell trains the model utilizing the loss function, optimizer, and number of epochs defined in the previous cell. The training loop also reports every `200` images the current epoch, the current image within the epoch, the total loss across the past `200` images, and the time elapsed since the model was trained on the last `200` images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bf9eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    i = 0\n",
    "    start_time = time.time()\n",
    "    cancer_net.train()\n",
    "    current_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cancer_net(inputs)\n",
    "        loss = loss_fn(outputs.view(-1), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_loss += loss.item()\n",
    "        if i % 100 == 99 or i == len(train_loader) - 1:\n",
    "            end_time = time.time()\n",
    "            print(f\"[Epoch: {epoch + 1}/{num_epochs}, Batch: {i + 1}/{len(train_loader)}] Loss: {current_loss:0.5f}, Time Elapsed: {end_time - start_time:0.5f}s\")\n",
    "            current_loss = 0.0\n",
    "            start_time = end_time\n",
    "        i += 1\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845cd145",
   "metadata": {},
   "source": [
    "#### Evaluate the Model\n",
    "The below cell block evaluates the model's **accuracy**, **precision**, **recall**, and **F1 score** with a strict threshold of `50%` confidence required to flag a tumor as malignant. Typical cancer applications would utilize a lower threshold to ensure that any false negatives don't slip through the cracks, but a stricter threshold is used here to overexaggerate how inaccurate the model is in order to further improve upon its development. Both the training and test datasets' performance is evaluated here, so as to check whether the model has overfitted or not.\n",
    "\n",
    "For this model, **recall** is the most important metric as it measures the percentage of positives that were identified correctly. Higher **recall** means that fewer positive results slipped through the cracks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c6b11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.4\n",
    "\n",
    "def evaluate_model(model, data_loader, threshold, data_name=\"dataset\"):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    total_positive = 0\n",
    "    predicted_positive = 0\n",
    "    predicted_positive_correct = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            predicted = torch.sigmoid(outputs.data)\n",
    "            predicted = (predicted > threshold).long()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total_positive += (labels == 1).sum().item()\n",
    "            predicted_positive += (predicted == 1).sum().item()\n",
    "            predicted_positive_correct += (predicted == labels and predicted == 1).sum().item()\n",
    "            \n",
    "    accuracy = correct/total\n",
    "    precision = predicted_positive_correct/predicted_positive\n",
    "    recall = predicted_positive_correct/total_positive\n",
    "    print(f\"{data_name.title()} Accuracy: {correct}/{total} => {accuracy:0.7f}\")\n",
    "    print(f\"{data_name.title()} Precision: {predicted_positive_correct}/{predicted_positive} => {precision:0.7f}\")\n",
    "    print(f\"{data_name.title()} Recall: {predicted_positive_correct}/{total_positive} => {recall:0.7f}\")\n",
    "    print(f\"{data_name.title()} F1 Score: {(2 * precision * recall/(precision + recall)):0.7f}\")\n",
    "\n",
    "evaluate_model(cancer_net, test_loader, threshold, \"test data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861916f9",
   "metadata": {},
   "source": [
    "#### Load Cross-Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f69fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download \"sayedmeeralishah/breast-cancer-segmentation-dataset-preprocessed\" -p \"./cross_validation\"\n",
    "!unzip cross_validation/breast-cancer-segmentation-dataset-preprocessed\n",
    "!mv \"Breast-canser_preprocessed dataset/benign\" cross_validation\n",
    "!mv \"Breast-canser_preprocessed dataset/malignant\" cross_validation\n",
    "!find cross_validation -type f -name \"*_mask.png\" -delete\n",
    "\n",
    "!rm -rf \"Breast-canser_preprocessed dataset\"\n",
    "!rm -rf cross_validation/breast-cancer-segmentation-dataset-preprocessed.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb8785a",
   "metadata": {},
   "source": [
    "#### Evaluate Against Cross-Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ae699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation_dataset = datasets.ImageFolder(root=\"cross_validation\", transform=transform)\n",
    "cross_validation_loader = DataLoader(cross_validation_dataset, batch_size=1, shuffle=False)\n",
    "evaluate_model(cancer_net, cross_validation_loader, threshold, \"cross validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619cb470",
   "metadata": {},
   "source": [
    "#### Save Weights\n",
    "Save the model's weights to a `.pth` file for later use, or in case the Jupyter Notebook kernel needs to be restarted to free up VRAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14a1469",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cancer_net.state_dict(), \"breast_cancer_cnn.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e734cc",
   "metadata": {},
   "source": [
    "#### Perform an Inference\n",
    "The following code cell loads an image (given a path to the image) into the model and performs an inference on it, returning a dictionary of the classification given to the image by the model and the model's confidence on that classification. The confidence is expressed as a decimal ranging from `0` to `1`. Multiplying it by `100` will result in a percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb26dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "from PIL import Image\n",
    "\n",
    "def infer(self, img_path: str, threshold):\n",
    "    self.eval()\n",
    "\n",
    "    inference_transforms = v2.Compose([\n",
    "        v2.Grayscale(num_output_channels=1),\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Normalize(mean=[0.5], std=[0.5]),\n",
    "    ])\n",
    "    with Image.open(img_path) as pillow_image:\n",
    "        # Unsqueeze adds an extra dimension to emulate a batch of size 1\n",
    "        image = inference_transforms(pillow_image).unsqueeze(0).to(device)\n",
    "        output = self(image)\n",
    "        prediction = torch.sigmoid(output.data).item()\n",
    "        if prediction > threshold:\n",
    "            return {\"label\": \"malignant\", \"confidence\": prediction}\n",
    "        else:\n",
    "            return {\"label\": \"benign\", \"confidence\": 1 - prediction}\n",
    "\n",
    "cancer_net.infer = types.MethodType(infer, cancer_net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b167984b",
   "metadata": {},
   "source": [
    "#### Example Inference\n",
    "The below code cell loads the saved model weights and performs an inference based on an image path from the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cb5bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_net.load_state_dict(torch.load(\"breast_cancer_cnn.pth\"))\n",
    "cancer_net.infer(\"data/benign/benign (1).png\", 0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
