{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KedarPanchal/Breast-Cancer-Detector/blob/main/tumor_detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdce3096",
      "metadata": {
        "id": "cdce3096"
      },
      "source": [
        "#### Python Version\n",
        "This neural network runs on Python 3.12 to ensure compatability with its dependencies. If you are running this notebook in a virtual environment, ensure you have the correct runtime selected by running the below cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac792483",
      "metadata": {
        "id": "ac792483"
      },
      "outputs": [],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99faf28f",
      "metadata": {
        "id": "99faf28f"
      },
      "source": [
        "#### Install Dependencies\n",
        "Installs the following dependencies for use in the notebook:\n",
        "* **Torch:** The model is built using the PyTorch framework (this is also what limits the Python version to <= 3.12)\n",
        "* **Torchvision:** Has functions for handling and preparing datasets for PyTorch models\n",
        "* **Opendatasets:** Download datasets from the Kaggle online repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6b2a6a2",
      "metadata": {
        "id": "e6b2a6a2"
      },
      "outputs": [],
      "source": [
        "%pip install torch\n",
        "%pip install torchvision\n",
        "%pip install opendatasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b90d44b",
      "metadata": {
        "id": "4b90d44b"
      },
      "source": [
        "#### Download and Prepare Datasets for Use\n",
        "> Prior to running this code block, ensure you have access to your Kaggle username and API Key, as the download will prompt you to enter this information. Visit the Kaggle website for information on how to acquire an API key.\n",
        "This neural network combines data from two datasets:\n",
        "* The Breast Ultrasound Images (BUSI) Dataset (Al-Dhabyani W, Gomaa M, Khaled H, Fahmy A. Dataset of breast ultrasound images. Data in Brief. 2020 Feb;28:104863. DOI: 10.1016/j.dib.2019.104863.)\n",
        "* Vuppala Adithya Sairam's Ultrasound Breat Images for Breast Cancer dataset, for which he has not provided a source other than the fact that it was aggregated from various open breast cancer ultrasound datasets\n",
        "\n",
        "The BUSI dataset had an additional \"normal\" class of ultrasounds that had no tumors, but these are deleted as the purpose of this model is to identify whether a detected tumor is malignant of benign. Both datasets have \"benign\" and \"malignant\" images which are aggregated together. Sairam's dataset was already split into test and evaluation datasets, but these were combined as this notebook randomly splits the datasets later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3f43d0f",
      "metadata": {
        "id": "a3f43d0f"
      },
      "outputs": [],
      "source": [
        "import opendatasets\n",
        "opendatasets.download(\"https://www.kaggle.com/datasets/aryashah2k/breast-ultrasound-images-dataset\")\n",
        "\n",
        "!mkdir data\n",
        "!rm -rf breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/normal\n",
        "!mv breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/* data\n",
        "!rm -rf breast-ultrasound-images-dataset\n",
        "!find data -type f -name \"*_mask*.png\" -delete\n",
        "\n",
        "opendatasets.download(\"https://kaggle.com/datasets/vuppalaadithyasairam/ultrasound-breast-images-for-breast-cancer\")\n",
        "!mv \"ultrasound-breast-images-for-breast-cancer/ultrasound breast classification/train/benign\"/* data/benign\n",
        "!mv \"ultrasound-breast-images-for-breast-cancer/ultrasound breast classification/val/benign\"/* data/benign\n",
        "!mv \"ultrasound-breast-images-for-breast-cancer/ultrasound breast classification/train/malignant\"/* data/malignant\n",
        "!mv \"ultrasound-breast-images-for-breast-cancer/ultrasound breast classification/val/malignant\"/* data/malignant\n",
        "!rm -rf \"ultrasound-breast-images-for-breast-cancer\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f775dd27",
      "metadata": {
        "id": "f775dd27"
      },
      "source": [
        "#### Import Necessary Dependencies\n",
        "The following dependencies are imported:\n",
        "* `torch`: Contains various functions for identifying GPUs and manipulating Tensors\n",
        "* `torch.nn`: Contains various prebuilt layers and classes to develop a deep learning network\n",
        "* `torch.nn.functional`: Contains implementations of activation functions that add non-linearity to a deep learning network\n",
        "* `torch.optim`: Contains optimizers used in model training\n",
        "* `time`: Used in displaying metrics while training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cee9d9a5",
      "metadata": {
        "id": "cee9d9a5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import models\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "391a2eb1",
      "metadata": {
        "id": "391a2eb1"
      },
      "source": [
        "#### Select Device for Training\n",
        "The following cell selects the best available device for training, testing, and performing inferences with the AI model. If a CUDA GPU is available, all calculations will be performed on the GPU. If an M-series Mac is used, PyTorch's MPS backend is used. Otherwise, all calculations will be done on the CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "de716683",
      "metadata": {
        "id": "de716683"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: mps\n"
          ]
        }
      ],
      "source": [
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif torch.backends.mps.is_available(): # This won't really work with 90% of the features on here but oh well!\n",
        "    device = \"mps\"\n",
        "\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a26644a8",
      "metadata": {},
      "source": [
        "#### Delete .DS_Store Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3005fb2",
      "metadata": {},
      "outputs": [],
      "source": [
        "!find . -name \".DS_Store\" -print -delete"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b3b7c54",
      "metadata": {
        "id": "7b3b7c54"
      },
      "source": [
        "#### Load and Transform Datasets\n",
        "The `torchvision` library is used to load and transform the data. The data is turned into a labeled dataset with the following labels:\n",
        "* Images in `data/benign` will have a label `0`\n",
        "* Images in `data/malignant` will have a label `1`\n",
        "\n",
        "Images in the dataset are also transformed. They are converted to Grayscale (ultrasounds are in black and white anyway, so training on 3 color channels is a waste of computation power), transformed to Tensor shapes, and normalized to have a mean and standard deviation of 0.5. The data set is then randomly split into a train and test dataset, with the train dataset containing `80%` of the original dataset and the test dataset containing the remaining `20%`. These two datasets are then loaded, with the training dataset being randomly shuffled every epoch.\n",
        "\n",
        "A batch size of `1` is used for both datasets as PyTorch expects batches to contain data of the same size. Since this model is designed to handle images of any size and the datasets are shuffled, a batch size of `1` is used to avoid any errors regarding data size mismatches within batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "176e3778",
      "metadata": {
        "id": "176e3778"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets\n",
        "from torchvision.transforms import v2\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "transform = v2.Compose([\n",
        "    v2.Grayscale(num_output_channels=1),\n",
        "    v2.RandomHorizontalFlip(0.5),\n",
        "    v2.RandomRotation(30),\n",
        "    v2.Resize((256, 256)),\n",
        "    v2.ToImage(),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    v2.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "dataset = datasets.ImageFolder(root=\"data\", transform=transform)\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, num_workers=4, shuffle=True, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b1bd2d8",
      "metadata": {
        "id": "2b1bd2d8"
      },
      "source": [
        "#### Identifying Dataset Biases\n",
        "In the source images, there are fewer malignant tumor images than benign tumor images. Since the training and test datasets are randomly split fractions of the total ultrasound dataset, it's safe to assume that in the training data there are more instances of benign tumors than malignant. This cell is meant to verify that assumption."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a6d197f",
      "metadata": {
        "id": "3a6d197f"
      },
      "outputs": [],
      "source": [
        "benign_count = 0\n",
        "malignant_count = 0\n",
        "for _, label in train_loader:\n",
        "    benign_count += label[label == 0].size(0)\n",
        "    malignant_count += label[label == 1].size(0)\n",
        "\n",
        "print(f\"Benign Image Count: {benign_count}\")\n",
        "print(f\"Malignant Image Count: {malignant_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2cd1848",
      "metadata": {
        "id": "f2cd1848"
      },
      "source": [
        "#### Addressing Dataset Biases\n",
        "Because there tend to be fewer examples of malignant tumors, it's harder for the neural network to identify malignant tumors compared to benign ones. Regular cross-entropy or binary cross-entropy loss functions don't address this issue, but their variant focal loss does. The focal loss formula looks like this:\n",
        "$$\n",
        "FL(p_t) = -\\alpha_t(1-p_t)^\\gamma\\log(p_t)\n",
        "$$\n",
        "\n",
        "$p_t =$ Probability of the input being of label $t$\n",
        "\n",
        "$\\alpha_t =$ Hyperparameter from $[0, 1]$ that scales down the loss of the label with fewer training instances. In binary classification tasks $\\alpha_t = \\alpha$ if $p_t = p$ and $\\alpha_t = (1 - \\alpha)$ if $p_t = (1 - p)$\n",
        "\n",
        "$\\gamma =$ Hyperparameter that is $\\geq 0$ that scales down the loss of easily identifiable labels to focus on training harder ones\n",
        "\n",
        "When adapted for binary classification tasks, the focal loss formula can look like the following:\n",
        "\n",
        "$$\n",
        "FL(p, y) = -\\alpha y(1-p)^\\gamma\\log(p) - (1 - \\alpha) (1-y)(p)^\\gamma\\log(1-p)\n",
        "$$\n",
        "$y =$ Whether the label is $1$ (malignant) or $0$ (benign)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a2c03661",
      "metadata": {
        "id": "a2c03661"
      },
      "outputs": [],
      "source": [
        "class BinaryFocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2.0, reduction=\"mean\"):\n",
        "        super(BinaryFocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        # Convert to float for Binary Cross Entropy\n",
        "        targets = targets.float()\n",
        "        cross_entropy_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
        "        # Sigmoid the inputs to convert them into probabilities\n",
        "        p = torch.sigmoid(inputs)\n",
        "        # Since targets is either 0 or 1, this returns the probability for each possible outcome as either targets or (1 - targets) is 0 in these equations\n",
        "        p_t = p * targets + (1 - p) * (1 - targets)\n",
        "        alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
        "        # Actually apply the focal loss formula\n",
        "        focal_loss = alpha_t * (1 - p_t) ** self.gamma * cross_entropy_loss\n",
        "\n",
        "        if self.reduction == \"mean\":\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == \"sum\":\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3daa1f8b",
      "metadata": {},
      "source": [
        "#### Define Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b53d43ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, optimizer, loss_fn, scheduler, num_epochs=20, device=device):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        i = 0\n",
        "        start_time = time.time()\n",
        "        current_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs.view(-1), labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            current_loss += loss.item()\n",
        "            if i % 100 == 99 or i == len(train_loader) - 1:\n",
        "                end_time = time.time()\n",
        "                print(f\"[Epoch: {epoch + 1}/{num_epochs}, Batch: {i + 1}/{len(train_loader)}] Loss: {current_loss:0.5f}, Time Elapsed: {end_time - start_time:0.5f}s\")\n",
        "                current_loss = 0.0\n",
        "                start_time = end_time\n",
        "            i += 1\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    print(\"Training Complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cc5fe6f",
      "metadata": {},
      "source": [
        "#### Define Evaluation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "168c6a25",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, data_loader, threshold, device=device, data_name=\"dataset\"):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    total_positive = 0\n",
        "    predicted_positive = 0\n",
        "    predicted_positive_correct = 0\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            predicted = torch.sigmoid(outputs.data)\n",
        "            predicted = (predicted > threshold).long()\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total_positive += (labels == 1).sum().item()\n",
        "            predicted_positive += (predicted == 1).sum().item()\n",
        "            predicted_positive_correct += (predicted == labels and predicted == 1).sum().item()\n",
        "\n",
        "    accuracy = correct/total\n",
        "    precision = predicted_positive_correct/predicted_positive\n",
        "    recall = predicted_positive_correct/total_positive\n",
        "    print(f\"{data_name.title()} Accuracy: {correct}/{total} => {accuracy:0.7f}\")\n",
        "    print(f\"{data_name.title()} Precision: {predicted_positive_correct}/{predicted_positive} => {precision:0.7f}\")\n",
        "    print(f\"{data_name.title()} Recall: {predicted_positive_correct}/{total_positive} => {recall:0.7f}\")\n",
        "    print(f\"{data_name.title()} F1 Score: {(2 * precision * recall/(precision + recall)):0.7f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32323d5e",
      "metadata": {},
      "source": [
        "#### Initialize ShuffleNetV2-0.5x Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "7c5a4f4d",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kpanchal/Documents/code/AI/Breast Cancer Project/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/Users/kpanchal/Documents/code/AI/Breast Cancer Project/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ShuffleNet_V2_X0_5_Weights.IMAGENET1K_V1`. You can also use `weights=ShuffleNet_V2_X0_5_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "shufflenet_model = models.shufflenet_v2_x0_5(pretrained=True)\n",
        "shufflenet_model.conv1 = nn.Sequential(\n",
        "    nn.Conv2d(1, 24, kernel_size=3, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(24),\n",
        "    nn.ReLU(inplace=True)\n",
        ")\n",
        "shufflenet_model.fc = nn.Sequential(\n",
        "    nn.Linear(shufflenet_model.fc.in_features, 512),\n",
        "    nn.LeakyReLU(0.01),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(512, 128),\n",
        "    nn.LeakyReLU(0.01),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(128, 1)    \n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98188913",
      "metadata": {},
      "source": [
        "#### Initialize ShuffleNetV2-0.5x Model Loss and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "51a25348",
      "metadata": {},
      "outputs": [],
      "source": [
        "shufflenet_model = shufflenet_model.to(device)\n",
        "loss_fn = BinaryFocalLoss(alpha=0.25, gamma=4)\n",
        "optimizer = optim.AdamW(shufflenet_model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "scheduler = lr_scheduler.CyclicLR(optimizer, base_lr=1e-4, max_lr=1e-3, step_size_up=2, mode=\"exp_range\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd74da82",
      "metadata": {},
      "source": [
        "#### Train the ShuffleNetV2-0.5x Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "760182b0",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kpanchal/Documents/code/AI/Breast Cancer Project/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch: 1/20, Batch: 100/115] Loss: 1.76352, Time Elapsed: 26.04651s\n",
            "[Epoch: 1/20, Batch: 115/115] Loss: 0.20721, Time Elapsed: 3.86218s\n",
            "[Epoch: 2/20, Batch: 100/115] Loss: 1.01294, Time Elapsed: 20.99370s\n",
            "[Epoch: 2/20, Batch: 115/115] Loss: 0.10794, Time Elapsed: 3.09680s\n",
            "[Epoch: 3/20, Batch: 100/115] Loss: 0.61595, Time Elapsed: 20.93535s\n",
            "[Epoch: 3/20, Batch: 115/115] Loss: 0.07109, Time Elapsed: 3.18380s\n",
            "[Epoch: 4/20, Batch: 100/115] Loss: 0.34741, Time Elapsed: 21.00638s\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshufflenet_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, optimizer, loss_fn, scheduler, num_epochs, device)\u001b[39m\n\u001b[32m      5\u001b[39m start_time = time.time()\n\u001b[32m      6\u001b[39m current_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/AI/Breast Cancer Project/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/AI/Breast Cancer Project/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/AI/Breast Cancer Project/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/AI/Breast Cancer Project/.venv/lib/python3.12/site-packages/torch/utils/data/dataset.py:416\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mAlbumentationsImageFolder.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     25\u001b[39m file_path = \u001b[38;5;28mself\u001b[39m.file_paths[idx]\n\u001b[32m     27\u001b[39m image = Image.open(file_path)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m image = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform:\n\u001b[32m     31\u001b[39m     transformed = \u001b[38;5;28mself\u001b[39m.transform(image=image)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/AI/Breast Cancer Project/.venv/lib/python3.12/site-packages/PIL/Image.py:735\u001b[39m, in \u001b[36mImage.__array_interface__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    733\u001b[39m     new[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.tobytes(\u001b[33m\"\u001b[39m\u001b[33mraw\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m735\u001b[39m     new[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    736\u001b[39m new[\u001b[33m\"\u001b[39m\u001b[33mshape\u001b[39m\u001b[33m\"\u001b[39m], new[\u001b[33m\"\u001b[39m\u001b[33mtypestr\u001b[39m\u001b[33m\"\u001b[39m] = _conv_type_shape(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    737\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/AI/Breast Cancer Project/.venv/lib/python3.12/site-packages/PIL/Image.py:794\u001b[39m, in \u001b[36mImage.tobytes\u001b[39m\u001b[34m(self, encoder_name, *args)\u001b[39m\n\u001b[32m    791\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m encoder_name == \u001b[33m\"\u001b[39m\u001b[33mraw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m encoder_args == ():\n\u001b[32m    792\u001b[39m     encoder_args = \u001b[38;5;28mself\u001b[39m.mode\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.width == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.height == \u001b[32m0\u001b[39m:\n\u001b[32m    797\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/AI/Breast Cancer Project/.venv/lib/python3.12/site-packages/PIL/ImageFile.py:389\u001b[39m, in \u001b[36mImageFile.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    386\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[32m    388\u001b[39m b = b + s\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m n, err_code = \u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n < \u001b[32m0\u001b[39m:\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "train_model(shufflenet_model, optimizer, loss_fn, scheduler, 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "407ed9da",
      "metadata": {},
      "source": [
        "#### Save ShuffleNetV2-0.5x Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5d5f797",
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(shufflenet_model.state_dict(), \"shufflenet_weights.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d1a3ecb",
      "metadata": {},
      "source": [
        "#### Evaluate the ShuffleNetV2-0.5x Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a33550d",
      "metadata": {},
      "outputs": [],
      "source": [
        "threshold = 0.3\n",
        "evaluate_model(shufflenet_model, test_loader, threshold, data_name=\"test data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf4e7d3f",
      "metadata": {},
      "source": [
        "#### Ensemble Part 3: Initialize _ Component"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "861916f9",
      "metadata": {
        "id": "861916f9"
      },
      "source": [
        "#### Load Cross-Validation Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55f69fb7",
      "metadata": {
        "id": "55f69fb7"
      },
      "outputs": [],
      "source": [
        "opendatasets.download(\"https://kaggle.com/datasets/sayedmeeralishah/breast-cancer-segmentation-dataset-preprocessed\")\n",
        "!mkdir \"cross_validation\"\n",
        "!mv \"breast-cancer-segmentation-dataset-preprocessed/Breast-canser_preprocessed dataset/benign\" cross_validation\n",
        "!mv \"breast-cancer-segmentation-dataset-preprocessed/Breast-canser_preprocessed dataset/malignant\" cross_validation\n",
        "!find cross_validation -type f -name \"*_mask.png\" -delete\n",
        "\n",
        "!rm -rf breast-cancer-segmentation-dataset-preprocessed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efb8785a",
      "metadata": {
        "id": "efb8785a"
      },
      "source": [
        "#### Evaluate Against Cross-Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1ae699f",
      "metadata": {
        "id": "e1ae699f"
      },
      "outputs": [],
      "source": [
        "## Change for ensemble cross-validation once ensemble is finalized\n",
        "cross_validation_transform = v2.Compose([\n",
        "    v2.Grayscale(num_output_channels=1),\n",
        "    v2.ToImage(),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    v2.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "cross_validation_dataset = datasets.ImageFolder(root=\"cross_validation\", transform=cross_validation_transform)\n",
        "cross_validation_loader = DataLoader(cross_validation_dataset, batch_size=1, shuffle=False)\n",
        "threshold = 0.3\n",
        "evaluate_model(shufflenet_model, cross_validation_loader, threshold, data_name=\"cross validation\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
