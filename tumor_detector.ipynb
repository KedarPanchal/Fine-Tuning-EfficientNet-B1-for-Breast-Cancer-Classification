{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KedarPanchal/Breast-Cancer-Detector/blob/main/tumor_detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdce3096",
      "metadata": {
        "id": "cdce3096"
      },
      "source": [
        "#### Python Version\n",
        "This neural network runs on Python 3.12 to ensure compatability with its dependencies. If you are running this notebook in a virtual environment, ensure you have the correct runtime selected by running the below cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac792483",
      "metadata": {
        "id": "ac792483"
      },
      "outputs": [],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99faf28f",
      "metadata": {
        "id": "99faf28f"
      },
      "source": [
        "#### Install Dependencies\n",
        "Installs the following dependencies for use in the notebook:\n",
        "* **Torch:** The model is built using the PyTorch framework (this is also what limits the Python version to <= 3.12)\n",
        "* **Torchvision:** Has functions for handling and preparing datasets for PyTorch models\n",
        "* **Opendatasets:** Download datasets from the Kaggle online repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6b2a6a2",
      "metadata": {
        "id": "e6b2a6a2"
      },
      "outputs": [],
      "source": [
        "%pip install torch\n",
        "%pip install torchvision\n",
        "%pip install opendatasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b90d44b",
      "metadata": {
        "id": "4b90d44b"
      },
      "source": [
        "#### Download and Prepare Datasets for Use\n",
        "> Prior to running this code block, ensure you have access to your Kaggle username and API Key, as the download will prompt you to enter this information. Visit the Kaggle website for information on how to acquire an API key.\n",
        "This neural network combines data from two datasets:\n",
        "* The Breast Ultrasound Images (BUSI) Dataset (Al-Dhabyani W, Gomaa M, Khaled H, Fahmy A. Dataset of breast ultrasound images. Data in Brief. 2020 Feb;28:104863. DOI: 10.1016/j.dib.2019.104863.)\n",
        "* Vuppala Adithya Sairam's Ultrasound Breat Images for Breast Cancer dataset, for which he has not provided a source other than the fact that it was aggregated from various open breast cancer ultrasound datasets\n",
        "\n",
        "The BUSI dataset had an additional \"normal\" class of ultrasounds that had no tumors, but these are deleted as the purpose of this model is to identify whether a detected tumor is malignant of benign. Both datasets have \"benign\" and \"malignant\" images which are aggregated together. Sairam's dataset was already split into test and evaluation datasets, but these were combined as this notebook randomly splits the datasets later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3f43d0f",
      "metadata": {
        "id": "a3f43d0f"
      },
      "outputs": [],
      "source": [
        "import opendatasets\n",
        "opendatasets.download(\"https://www.kaggle.com/datasets/aryashah2k/breast-ultrasound-images-dataset\")\n",
        "\n",
        "!mkdir data\n",
        "!rm -rf breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/normal\n",
        "!mv breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/* data\n",
        "!rm -rf breast-ultrasound-images-dataset\n",
        "!find data -type f -name \"*_mask*.png\" -delete\n",
        "\n",
        "opendatasets.download(\"https://kaggle.com/datasets/vuppalaadithyasairam/ultrasound-breast-images-for-breast-cancer\")\n",
        "!mv \"ultrasound-breast-images-for-breast-cancer/ultrasound breast classification/train/benign\"/* data/benign\n",
        "!mv \"ultrasound-breast-images-for-breast-cancer/ultrasound breast classification/val/benign\"/* data/benign\n",
        "!mv \"ultrasound-breast-images-for-breast-cancer/ultrasound breast classification/train/malignant\"/* data/malignant\n",
        "!mv \"ultrasound-breast-images-for-breast-cancer/ultrasound breast classification/val/malignant\"/* data/malignant\n",
        "!rm -rf \"ultrasound-breast-images-for-breast-cancer\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f775dd27",
      "metadata": {
        "id": "f775dd27"
      },
      "source": [
        "#### Import Necessary Dependencies\n",
        "The following dependencies are imported:\n",
        "* `torch`: Contains various functions for identifying GPUs and manipulating Tensors\n",
        "* `torch.nn`: Contains various prebuilt layers and classes to develop a deep learning network\n",
        "* `torch.nn.functional`: Contains implementations of activation functions that add non-linearity to a deep learning network\n",
        "* `torch.optim`: Contains optimizers used in model training\n",
        "* `time`: Used in displaying metrics while training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cee9d9a5",
      "metadata": {
        "id": "cee9d9a5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "391a2eb1",
      "metadata": {
        "id": "391a2eb1"
      },
      "source": [
        "#### Select Device for Training\n",
        "The following cell selects the best available device for training, testing, and performing inferences with the AI model. If a CUDA GPU is available, all calculations will be performed on the GPU. If an M-series Mac is used, PyTorch's MPS backend is used. Otherwise, all calculations will be done on the CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de716683",
      "metadata": {
        "id": "de716683"
      },
      "outputs": [],
      "source": [
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif torch.backends.mps.is_available(): # This won't really work with 90% of the features on here but oh well!\n",
        "    device = \"mps\"\n",
        "\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b3b7c54",
      "metadata": {
        "id": "7b3b7c54"
      },
      "source": [
        "#### Load and Transform Datasets\n",
        "The `torchvision` library is used to load and transform the data. The data is turned into a labeled dataset with the following labels:\n",
        "* Images in `data/benign` will have a label `0`\n",
        "* Images in `data/malignant` will have a label `1`\n",
        "\n",
        "Images in the dataset are also transformed. They are converted to Grayscale (ultrasounds are in black and white anyway, so training on 3 color channels is a waste of computation power), transformed to Tensor shapes, and normalized to have a mean and standard deviation of 0.5. The data set is then randomly split into a train and test dataset, with the train dataset containing `80%` of the original dataset and the test dataset containing the remaining `20%`. These two datasets are then loaded, with the training dataset being randomly shuffled every epoch.\n",
        "\n",
        "A batch size of `1` is used for both datasets as PyTorch expects batches to contain data of the same size. Since this model is designed to handle images of any size and the datasets are shuffled, a batch size of `1` is used to avoid any errors regarding data size mismatches within batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "176e3778",
      "metadata": {
        "id": "176e3778"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets\n",
        "from torchvision.transforms import v2\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "transform = v2.Compose([\n",
        "    v2.Grayscale(num_output_channels=1),\n",
        "    v2.RandomHorizontalFlip(0.5),\n",
        "    v2.RandomRotation(30),\n",
        "    v2.Resize((256, 256)),\n",
        "    v2.ToImage(),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    v2.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "dataset = datasets.ImageFolder(root=\"data\", transform=transform)\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, num_workers=4, shuffle=True, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b1bd2d8",
      "metadata": {
        "id": "2b1bd2d8"
      },
      "source": [
        "#### Identifying Dataset Biases\n",
        "In the source images, there are fewer malignant tumor images than benign tumor images. Since the training and test datasets are randomly split fractions of the total ultrasound dataset, it's safe to assume that in the training data there are more instances of benign tumors than malignant. This cell is meant to verify that assumption."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a6d197f",
      "metadata": {
        "id": "3a6d197f"
      },
      "outputs": [],
      "source": [
        "benign_count = 0\n",
        "malignant_count = 0\n",
        "for _, label in train_loader:\n",
        "    benign_count += label[label == 0].size(0)\n",
        "    malignant_count += label[label == 1].size(0)\n",
        "\n",
        "print(f\"Benign Image Count: {benign_count}\")\n",
        "print(f\"Malignant Image Count: {malignant_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2cd1848",
      "metadata": {
        "id": "f2cd1848"
      },
      "source": [
        "#### Addressing Dataset Biases\n",
        "Because there tend to be fewer examples of malignant tumors, it's harder for the neural network to identify malignant tumors compared to benign ones. Regular cross-entropy or binary cross-entropy loss functions don't address this issue, but their variant focal loss does. The focal loss formula looks like this:\n",
        "$$\n",
        "FL(p_t) = -\\alpha_t(1-p_t)^\\gamma\\log(p_t)\n",
        "$$\n",
        "\n",
        "$p_t =$ Probability of the input being of label $t$\n",
        "\n",
        "$\\alpha_t =$ Hyperparameter from $[0, 1]$ that scales down the loss of the label with fewer training instances. In binary classification tasks $\\alpha_t = \\alpha$ if $p_t = p$ and $\\alpha_t = (1 - \\alpha)$ if $p_t = (1 - p)$\n",
        "\n",
        "$\\gamma =$ Hyperparameter that is $\\geq 0$ that scales down the loss of easily identifiable labels to focus on training harder ones\n",
        "\n",
        "When adapted for binary classification tasks, the focal loss formula can look like the following:\n",
        "\n",
        "$$\n",
        "FL(p, y) = -\\alpha y(1-p)^\\gamma\\log(p) - (1 - \\alpha) (1-y)(p)^\\gamma\\log(1-p)\n",
        "$$\n",
        "$y =$ Whether the label is $1$ (malignant) or $0$ (benign)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2c03661",
      "metadata": {
        "id": "a2c03661"
      },
      "outputs": [],
      "source": [
        "class BinaryFocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2.0, reduction=\"mean\"):\n",
        "        super(BinaryFocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        # Convert to float for Binary Cross Entropy\n",
        "        targets = targets.float()\n",
        "        cross_entropy_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
        "        # Sigmoid the inputs to convert them into probabilities\n",
        "        p = torch.sigmoid(inputs)\n",
        "        # Since targets is either 0 or 1, this returns the probability for each possible outcome as either targets or (1 - targets) is 0 in these equations\n",
        "        p_t = p * targets + (1 - p) * (1 - targets)\n",
        "        alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
        "        # Actually apply the focal loss formula\n",
        "        focal_loss = alpha_t * (1 - p_t) ** self.gamma * cross_entropy_loss\n",
        "\n",
        "        if self.reduction == \"mean\":\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == \"sum\":\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92b14ee8",
      "metadata": {},
      "source": [
        "#### Ensemble Part 1: Initialize ResNet18 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccd8a161",
      "metadata": {},
      "outputs": [],
      "source": [
        "resnet_component = models.resnet18(pretrained=True)\n",
        "resnet_component.fc = nn.Sequential(\n",
        "    nn.Linear(resnet_component.fc.in_features, 128),\n",
        "    nn.LeakyReLU(0.01),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(128, 1)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24d468b8",
      "metadata": {},
      "source": [
        "#### Initialize ResNet18 Model Loss, Optimizer, and Epoch Counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbdaea6f",
      "metadata": {},
      "outputs": [],
      "source": [
        "num_epochs = 10\n",
        "resnet_component = resnet_component.to(device)\n",
        "loss_fn = BinaryFocalLoss(alpha=0.8, gamma=2)\n",
        "optimizer = optim.AdamW(resnet_component.parameters(), lr=1e-3, weight_decay=1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f9060c7",
      "metadata": {},
      "source": [
        "#### Train the ResNet18 Component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "778b2702",
      "metadata": {},
      "outputs": [],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    i = 0\n",
        "    start_time = time.time()\n",
        "    resnet_component.train()\n",
        "    current_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = resnet_component(inputs)\n",
        "        loss = loss_fn(outputs.view(-1), labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        current_loss += loss.item()\n",
        "        if i % 100 == 99 or i == len(train_loader) - 1:\n",
        "            end_time = time.time()\n",
        "            print(f\"[Epoch: {epoch + 1}/{num_epochs}, Batch: {i + 1}/{len(train_loader)}] Loss: {current_loss:0.5f}, Time Elapsed: {end_time - start_time:0.5f}s\")\n",
        "            current_loss = 0.0\n",
        "            start_time = end_time\n",
        "        i += 1\n",
        "\n",
        "print(\"Training Complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba96ebcd",
      "metadata": {},
      "source": [
        "#### Save ResNet18 Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6294d64d",
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(resnet_component.state_dict(), \"restnet18_weights.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6a17ed7",
      "metadata": {},
      "source": [
        "#### Evaluate the Resnet18 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e89b527",
      "metadata": {},
      "outputs": [],
      "source": [
        "threshold = 0.3\n",
        "\n",
        "def evaluate_model(model, data_loader, threshold, data_name=\"dataset\"):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    total_positive = 0\n",
        "    predicted_positive = 0\n",
        "    predicted_positive_correct = 0\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            predicted = torch.sigmoid(outputs.data)\n",
        "            predicted = (predicted > threshold).long()\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total_positive += (labels == 1).sum().item()\n",
        "            predicted_positive += (predicted == 1).sum().item()\n",
        "            predicted_positive_correct += (predicted == labels and predicted == 1).sum().item()\n",
        "\n",
        "    accuracy = correct/total\n",
        "    precision = predicted_positive_correct/predicted_positive\n",
        "    recall = predicted_positive_correct/total_positive\n",
        "    print(f\"{data_name.title()} Accuracy: {correct}/{total} => {accuracy:0.7f}\")\n",
        "    print(f\"{data_name.title()} Precision: {predicted_positive_correct}/{predicted_positive} => {precision:0.7f}\")\n",
        "    print(f\"{data_name.title()} Recall: {predicted_positive_correct}/{total_positive} => {recall:0.7f}\")\n",
        "    print(f\"{data_name.title()} F1 Score: {(2 * precision * recall/(precision + recall)):0.7f}\")\n",
        "\n",
        "evaluate_model(resnet_component, test_loader, threshold, \"test data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "861916f9",
      "metadata": {
        "id": "861916f9"
      },
      "source": [
        "#### Load Cross-Validation Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55f69fb7",
      "metadata": {
        "id": "55f69fb7"
      },
      "outputs": [],
      "source": [
        "opendatasets.download(\"https://kaggle.com/datasets/sayedmeeralishah/breast-cancer-segmentation-dataset-preprocessed\")\n",
        "!mkdir \"cross_validation\"\n",
        "!mv \"breast-cancer-segmentation-dataset-preprocessed/Breast-canser_preprocessed dataset/benign\" cross_validation\n",
        "!mv \"breast-cancer-segmentation-dataset-preprocessed/Breast-canser_preprocessed dataset/malignant\" cross_validation\n",
        "!find cross_validation -type f -name \"*_mask.png\" -delete\n",
        "\n",
        "!rm -rf breast-cancer-segmentation-dataset-preprocessed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efb8785a",
      "metadata": {
        "id": "efb8785a"
      },
      "source": [
        "#### Evaluate Against Cross-Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1ae699f",
      "metadata": {
        "id": "e1ae699f"
      },
      "outputs": [],
      "source": [
        "## Change for ensemble cross-validation once ensemble is finalized\n",
        "cross_validation_transform = v2.Compose([\n",
        "    v2.Grayscale(num_output_channels=1),\n",
        "    v2.ToImage(),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    v2.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "cross_validation_dataset = datasets.ImageFolder(root=\"cross_validation\", transform=cross_validation_transform)\n",
        "cross_validation_loader = DataLoader(cross_validation_dataset, batch_size=1, shuffle=False)\n",
        "evaluate_model(cancer_net, cross_validation_loader, threshold, \"cross validation\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
