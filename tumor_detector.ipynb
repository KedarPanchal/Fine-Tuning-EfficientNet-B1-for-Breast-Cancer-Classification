{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KedarPanchal/Breast-Cancer-Detector/blob/main/tumor_detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdce3096",
      "metadata": {
        "id": "cdce3096"
      },
      "source": [
        "#### Python Version\n",
        "This neural network runs on Python 3.12 to ensure compatability with its dependencies. If you are running this notebook in a virtual environment, ensure you have the correct runtime selected by running the below cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac792483",
      "metadata": {
        "id": "ac792483"
      },
      "outputs": [],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99faf28f",
      "metadata": {
        "id": "99faf28f"
      },
      "source": [
        "#### Install Dependencies\n",
        "Installs the following dependencies for use in the notebook:\n",
        "* **Torch:** The model is built using the PyTorch framework (this is also what limits the Python version to <= 3.12)\n",
        "* **Torchvision:** Has functions for handling and preparing datasets for PyTorch models\n",
        "* **Opendatasets:** Download datasets from the Kaggle online repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6b2a6a2",
      "metadata": {
        "id": "e6b2a6a2"
      },
      "outputs": [],
      "source": [
        "%pip install torch\n",
        "%pip install torchvision\n",
        "%pip install opendatasets\n",
        "%pip install scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b90d44b",
      "metadata": {
        "id": "4b90d44b"
      },
      "source": [
        "#### Download and Prepare Datasets for Use\n",
        "> Prior to running this code block, ensure you have access to your Kaggle username and API Key, as the download will prompt you to enter this information. Visit the Kaggle website for information on how to acquire an API key.\n",
        "This neural network combines data from two datasets:\n",
        "* The Breast Ultrasound Images (BUSI) Dataset (Al-Dhabyani W, Gomaa M, Khaled H, Fahmy A. Dataset of breast ultrasound images. Data in Brief. 2020 Feb;28:104863. DOI: 10.1016/j.dib.2019.104863.)\n",
        "* Vuppala Adithya Sairam's Ultrasound Breat Images for Breast Cancer dataset, for which he has not provided a source other than the fact that it was aggregated from various open breast cancer ultrasound datasets\n",
        "\n",
        "The BUSI dataset had an additional \"normal\" class of ultrasounds that had no tumors, but these are deleted as the purpose of this model is to identify whether a detected tumor is malignant of benign. Both datasets have \"benign\" and \"malignant\" images which are aggregated together. Sairam's dataset was already split into test and evaluation datasets, but these were combined as this notebook randomly splits the datasets later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3f43d0f",
      "metadata": {
        "id": "a3f43d0f"
      },
      "outputs": [],
      "source": [
        "import opendatasets\n",
        "opendatasets.download(\"https://www.kaggle.com/datasets/aryashah2k/breast-ultrasound-images-dataset\")\n",
        "\n",
        "!mkdir data\n",
        "!rm -rf breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/normal\n",
        "!mv breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/* data\n",
        "!rm -rf breast-ultrasound-images-dataset\n",
        "!find data -type f -name \"*_mask*.png\" -delete"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f775dd27",
      "metadata": {
        "id": "f775dd27"
      },
      "source": [
        "#### Import Necessary Dependencies\n",
        "The following dependencies are imported:\n",
        "* `torch`: Contains various functions for identifying GPUs and manipulating Tensors\n",
        "* `torch.nn`: Contains various prebuilt layers and classes to develop a deep learning network\n",
        "* `torch.nn.functional`: Contains implementations of activation functions that add non-linearity to a deep learning network\n",
        "* `torch.optim`: Contains optimizers used in model training\n",
        "* `time`: Used in displaying metrics while training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cee9d9a5",
      "metadata": {
        "id": "cee9d9a5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import models\n",
        "import time\n",
        "import copy\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "391a2eb1",
      "metadata": {
        "id": "391a2eb1"
      },
      "source": [
        "#### Select Device for Training\n",
        "The following cell selects the best available device for training, testing, and performing inferences with the AI model. If a CUDA GPU is available, all calculations will be performed on the GPU. If an M-series Mac is used, PyTorch's MPS backend is used. Otherwise, all calculations will be done on the CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de716683",
      "metadata": {
        "id": "de716683"
      },
      "outputs": [],
      "source": [
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif torch.backends.mps.is_available(): # This won't really work with 90% of the features on here but oh well!\n",
        "    device = \"mps\"\n",
        "\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a26644a8",
      "metadata": {},
      "source": [
        "#### Delete .DS_Store Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3005fb2",
      "metadata": {},
      "outputs": [],
      "source": [
        "!find . -name \".DS_Store\" -print -delete"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b3b7c54",
      "metadata": {
        "id": "7b3b7c54"
      },
      "source": [
        "#### Initialize and Transform Datasets\n",
        "The `torchvision` library is used to load and transform the data. The data is turned into a labeled dataset with the following labels:\n",
        "* Images in `data/benign` will have a label `0`\n",
        "* Images in `data/malignant` will have a label `1`\n",
        "\n",
        "Images in the dataset are also transformed. They are converted to Grayscale (ultrasounds are in black and white anyway, so training on 3 color channels is a waste of computation power), transformed to Tensor shapes, and normalized to have a mean and standard deviation of 0.5. The data set is then randomly split into a train and test dataset, with the train dataset containing `80%` of the original dataset and the test dataset containing the remaining `20%`. These two datasets are then loaded, with the training dataset being randomly shuffled every epoch.\n",
        "\n",
        "A batch size of `1` is used for both datasets as PyTorch expects batches to contain data of the same size. Since this model is designed to handle images of any size and the datasets are shuffled, a batch size of `1` is used to avoid any errors regarding data size mismatches within batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "176e3778",
      "metadata": {
        "id": "176e3778"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "transform = v2.Compose([\n",
        "    v2.Grayscale(num_output_channels=1),\n",
        "    v2.RandomHorizontalFlip(0.5),\n",
        "    v2.RandomRotation(20),\n",
        "    v2.RandomAutocontrast(0.3),\n",
        "    v2.RandomAdjustSharpness(0.3),\n",
        "    v2.RandomEqualize(0.2),\n",
        "\n",
        "    v2.Resize((224, 224)),\n",
        "\n",
        "    v2.ToImage(),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    v2.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "dataset = datasets.ImageFolder(root=\"data\", transform=transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3daa1f8b",
      "metadata": {},
      "source": [
        "#### Define Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b53d43ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, data_loader, optimizer, loss_fn, scheduler, current_fold, num_epochs=20, device=device):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        i = 0\n",
        "        start_time = time.time()\n",
        "        current_loss = 0.0\n",
        "\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs.view(-1), labels.float())\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            current_loss += loss.item()\n",
        "            if i % 10 == 9 or i == len(data_loader) - 1:\n",
        "                end_time = time.time()\n",
        "                print(f\"[Fold: {current_fold + 1}, Epoch: {epoch + 1}/{num_epochs}, Batch: {i + 1}/{len(data_loader)}] Loss: {current_loss:0.5f}, Time Elapsed: {end_time - start_time:0.5f}s\")\n",
        "                current_loss = 0.\n",
        "                start_time = end_time\n",
        "            i += 1\n",
        "\n",
        "\n",
        "    print(\"Training Complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cc5fe6f",
      "metadata": {},
      "source": [
        "#### Define Evaluation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "168c6a25",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, data_loader, threshold, device=device):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    total_positive = 0\n",
        "    predicted_positive = 0\n",
        "    predicted_positive_correct = 0\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            predicted = torch.sigmoid(outputs.data)\n",
        "            predicted = (predicted > threshold).long()\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total_positive += (labels == 1).sum().item()\n",
        "            predicted_positive += (predicted == 1).sum().item()\n",
        "            predicted_positive_correct += (predicted == labels and predicted == 1).sum().item()\n",
        "\n",
        "    accuracy = correct/total\n",
        "    precision = predicted_positive_correct/predicted_positive\n",
        "    recall = predicted_positive_correct/total_positive\n",
        "    f1_score = 2 * precision * recall/(precision + recall)\n",
        "    return (accuracy, precision, recall, f1_score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32323d5e",
      "metadata": {},
      "source": [
        "#### Initialize ShuffleNetV2-0.5x Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c5a4f4d",
      "metadata": {},
      "outputs": [],
      "source": [
        "cancer_net = models.efficientnet_b1(weights=models.EfficientNet_B1_Weights.DEFAULT)\n",
        "cancer_net.features[0] = nn.Sequential(\n",
        "    nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(32),\n",
        "    nn.SiLU(inplace=True)\n",
        ")\n",
        "cancer_net.classifier = nn.Sequential(\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(cancer_net.classifier[1].in_features, 512),\n",
        "    nn.SiLU(inplace=True),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(512, 128),\n",
        "    nn.SiLU(inplace=True),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(128, 1)    \n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98188913",
      "metadata": {},
      "source": [
        "#### Initialize ShuffleNetV2-0.5x Model Loss and State Dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51a25348",
      "metadata": {},
      "outputs": [],
      "source": [
        "num_epochs = 10\n",
        "cancer_net = cancer_net.to(device)\n",
        "loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([0.8 * len(os.listdir(\"./data/benign\"))/len(os.listdir(\"./data/malignant\"))]).to(device))\n",
        "state_dict = copy.deepcopy(cancer_net.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd74da82",
      "metadata": {},
      "source": [
        "#### Train the ShuffleNetV2-0.5x Model Using K-Fold Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "760182b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "\n",
        "folds = 5\n",
        "batch_size = 32\n",
        "\n",
        "k_fold = KFold(n_splits=folds, shuffle=True)\n",
        "for fold, (train_i, test_i) in enumerate(k_fold.split(dataset)):\n",
        "    cancer_net.load_state_dict(state_dict)\n",
        "    optimizer = optim.AdamW(cancer_net.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "    scheduler = lr_scheduler.CyclicLR(optimizer, base_lr=3e-4, max_lr=1e-3, step_size_up=2, mode=\"exp_range\")\n",
        "    train_loader = DataLoader(dataset=dataset, batch_size=batch_size, sampler=SubsetRandomSampler(train_i))\n",
        "    test_loader = DataLoader(dataset=dataset, batch_size=1, sampler=SubsetRandomSampler(test_i))\n",
        "\n",
        "    train_model(cancer_net, train_loader, optimizer, loss_fn, scheduler, current_fold=fold, num_epochs=num_epochs)\n",
        "    accuracy, precision, recall, f1_score = evaluate_model(cancer_net, data_loader=test_loader, threshold=0.3)\n",
        "    print(f\"Test Accuracy: {accuracy:0.7f}\")\n",
        "    print(f\"Test Precision: {precision:0.7f}\")\n",
        "    print(f\"Test Recall: {recall:0.7f}\")\n",
        "    print(f\"Test F1 Score: {f1_score:0.7f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "407ed9da",
      "metadata": {},
      "source": [
        "#### Save ShuffleNetV2-0.5x Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5d5f797",
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(cancer_net.state_dict(), \"shufflenet_weights.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "861916f9",
      "metadata": {
        "id": "861916f9"
      },
      "source": [
        "#### Load Cross-Validation Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55f69fb7",
      "metadata": {
        "id": "55f69fb7"
      },
      "outputs": [],
      "source": [
        "import opendatasets\n",
        "opendatasets.download(\"https://kaggle.com/datasets/fhabibimoghaddam/breast-ultrasound-images\")\n",
        "!mkdir \"cross_validation\"\n",
        "!mv \"breast-ultrasound-images/Breast Ultrasound Images Dataset/benign\" cross_validation\n",
        "!mv \"breast-ultrasound-images/Breast Ultrasound Images Dataset/malignant\" cross_validation\n",
        "\n",
        "!rm -rf breast-ultrasound-images"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efb8785a",
      "metadata": {
        "id": "efb8785a"
      },
      "source": [
        "#### Evaluate Against Cross-Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1ae699f",
      "metadata": {
        "id": "e1ae699f"
      },
      "outputs": [],
      "source": [
        "## Change for ensemble cross-validation once ensemble is finalized\n",
        "cross_validation_transform = v2.Compose([\n",
        "    v2.Grayscale(num_output_channels=1),\n",
        "    v2.ToImage(),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    v2.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "cross_validation_dataset = datasets.ImageFolder(root=\"cross_validation\", transform=cross_validation_transform)\n",
        "cross_validation_loader = DataLoader(cross_validation_dataset, batch_size=1, shuffle=False)\n",
        "threshold = 0.3\n",
        "evaluate_model(cancer_net, cross_validation_loader, threshold, data_name=\"cross validation\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
